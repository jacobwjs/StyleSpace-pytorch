{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ab813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stylegan2.models import Generator\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "config = {\"latent\" : 512, \"n_mlp\" : 8, \"channel_multiplier\": 2}\n",
    "generator = Generator(\n",
    "        size= 1024,\n",
    "        style_dim=config[\"latent\"],\n",
    "        n_mlp=config[\"n_mlp\"],\n",
    "        channel_multiplier=config[\"channel_multiplier\"]\n",
    "    )\n",
    "\n",
    "generator.load_state_dict(torch.load(\"./pretrained/stylegan2-ffhq-config-f.pt\")['g_ema'])\n",
    "generator.eval()\n",
    "generator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a0857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "index = [0,1,1,2,2,3,4,4,5,6,6,7,8,8,9,10,10,11,12,12,13,14,14,15,16,16]\n",
    "\n",
    "def conv_warper(layer, input, style, noise):\n",
    "    # the conv should change\n",
    "    conv = layer.conv\n",
    "    batch, in_channel, height, width = input.shape\n",
    "\n",
    "    style = style.view(batch, 1, in_channel, 1, 1)\n",
    "    weight = conv.scale * conv.weight * style\n",
    "\n",
    "    if conv.demodulate:\n",
    "        demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4]) + 1e-8)\n",
    "        weight = weight * demod.view(batch, conv.out_channel, 1, 1, 1)\n",
    "\n",
    "    weight = weight.view(\n",
    "        batch * conv.out_channel, in_channel, conv.kernel_size, conv.kernel_size\n",
    "    )\n",
    "\n",
    "    if conv.upsample:\n",
    "        input = input.view(1, batch * in_channel, height, width)\n",
    "        weight = weight.view(\n",
    "            batch, conv.out_channel, in_channel, conv.kernel_size, conv.kernel_size\n",
    "        )\n",
    "        weight = weight.transpose(1, 2).reshape(\n",
    "            batch * in_channel, conv.out_channel, conv.kernel_size, conv.kernel_size\n",
    "        )\n",
    "        out = F.conv_transpose2d(input, weight, padding=0, stride=2, groups=batch)\n",
    "        _, _, height, width = out.shape\n",
    "        out = out.view(batch, conv.out_channel, height, width)\n",
    "        out = conv.blur(out)\n",
    "\n",
    "    elif conv.downsample:\n",
    "        input = conv.blur(input)\n",
    "        _, _, height, width = input.shape\n",
    "        input = input.view(1, batch * in_channel, height, width)\n",
    "        out = F.conv2d(input, weight, padding=0, stride=2, groups=batch)\n",
    "        _, _, height, width = out.shape\n",
    "        out = out.view(batch, conv.out_channel, height, width)\n",
    "\n",
    "    else:\n",
    "        input = input.view(1, batch * in_channel, height, width)\n",
    "        out = F.conv2d(input, weight, padding=conv.padding, groups=batch)\n",
    "        _, _, height, width = out.shape\n",
    "        out = out.view(batch, conv.out_channel, height, width)\n",
    "        \n",
    "    out = layer.noise(out, noise=noise)\n",
    "    out = layer.activate(out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def decoder(G, style_space, latent, noise):\n",
    "    # an decoder warper for G\n",
    "    out = G.input(latent)\n",
    "    out = conv_warper(G.conv1, out, style_space[0], noise[0])\n",
    "    skip = G.to_rgb1(out, latent[:, 1])\n",
    "\n",
    "    i = 1\n",
    "    for conv1, conv2, noise1, noise2, to_rgb in zip(\n",
    "        G.convs[::2], G.convs[1::2], noise[1::2], noise[2::2], G.to_rgbs\n",
    "    ):\n",
    "        out = conv_warper(conv1, out, style_space[i], noise=noise1)\n",
    "        out = conv_warper(conv2, out, style_space[i+1], noise=noise2)\n",
    "        skip = to_rgb(out, latent[:, i + 2], skip)\n",
    "\n",
    "        i += 2\n",
    "\n",
    "    image = skip\n",
    "\n",
    "    return image\n",
    "\n",
    "def encoder(G, noise):\n",
    "    # an encoder warper for G\n",
    "    styles = [noise]\n",
    "    style_space = []\n",
    "    \n",
    "    styles = [G.style(s) for s in styles]\n",
    "    noise = [getattr(G.noises, 'noise_{}'.format(i)) for i in range(G.num_layers)]\n",
    "    inject_index = G.n_latent\n",
    "    latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n",
    "    style_space.append(G.conv1.conv.modulation(latent[:, 0]))\n",
    "\n",
    "    i = 1\n",
    "    for conv1, conv2, noise1, noise2, to_rgb in zip(\n",
    "        G.convs[::2], G.convs[1::2], noise[1::2], noise[2::2], G.to_rgbs\n",
    "    ):\n",
    "        style_space.append(conv1.conv.modulation(latent[:, i]))\n",
    "        style_space.append(conv2.conv.modulation(latent[:, i+1]))\n",
    "        i += 2\n",
    "        \n",
    "    return style_space, latent, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbfb521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual(output):\n",
    "    output = (output + 1)/2\n",
    "    output = torch.clamp(output, 0, 1)\n",
    "    if output.shape[1] == 1:\n",
    "        output = torch.cat([output, output, output], 1)\n",
    "    output = output[0].detach().cpu().permute(1,2,0).numpy()\n",
    "    output = (output*255).astype(np.uint8)\n",
    "    plt.imshow(output)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36257c23",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420fc5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randn(1,512).cuda()\n",
    "\n",
    "output, _ = generator([test_input], False)\n",
    "visual(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032a988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 9_409 in the paper, you should have \"style_space[index[9]][:, 409]\"\n",
    "# the value to shift is hand-craft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed9aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eye\n",
    "style_space, latent, noise = encoder(generator, test_input)\n",
    "style_space[index[9]][:, 409] += 10\n",
    "image = decoder(generator, style_space, latent, noise)\n",
    "visual(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04590850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hair\n",
    "style_space, latent, noise = encoder(generator, test_input)\n",
    "# style_space[index[12]][:, 330] -= 10\n",
    "style_space[index[12]][:, 330] += 40\n",
    "image = decoder(generator, style_space, latent, noise)\n",
    "visual(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a94bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mouth\n",
    "style_space, latent, noise = encoder(generator, test_input)\n",
    "style_space[index[6]][:, 259] -= 10\n",
    "image = decoder(generator, style_space, latent, noise)\n",
    "visual(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae76f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lip\n",
    "style_space, latent, noise = encoder(generator, test_input)\n",
    "style_space[index[15]][:, 45] -= 10\n",
    "image = decoder(generator, style_space, latent, noise)\n",
    "visual(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ad5de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "087f98b4",
   "metadata": {},
   "source": [
    "## Can we replace StyleSpace values from A to B to match desired style of A and B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(1,512).cuda()\n",
    "B = torch.randn(1,512).cuda()\n",
    "\n",
    "output_A, _ = generator([A], False)\n",
    "output_B, _ = generator([B], False)\n",
    "\n",
    "visual(output_A)\n",
    "visual(output_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6836094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_space_A, latent_A, noise_A = encoder(generator, A)\n",
    "style_space_B, latent_B, noise_B = encoder(generator, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb1074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eye\n",
    "# ---------\n",
    "# style_space_B[index[9]][:, 409] += 10\n",
    "idx = 9\n",
    "\n",
    "style_space_B[index[idx]][:, 409] = style_space_A[index[idx]][:, 409]*-1 #+ 40.0\n",
    "image = decoder(generator, style_space_B, latent_B, noise_B)\n",
    "visual(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4758a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hair\n",
    "# ------------\n",
    "style_space_B, latent_B, noise_B = encoder(generator, B)\n",
    "\n",
    "idx = 12\n",
    "style = 330\n",
    "style_space_B[index[idx]][:, style] = style_space_A[index[idx]][:, style]*10*1\n",
    "image = decoder(generator, style_space_B, latent_B, noise_B)\n",
    "visual(image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df10394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grey hair\n",
    "# ------------\n",
    "style_space_B, latent_B, noise_B = encoder(generator, B)\n",
    "\n",
    "idx = 11\n",
    "style = 286\n",
    "style_space_B[index[idx]][:, style] -= 40 #style_space_A[index[idx]][:, style]*40*-1\n",
    "image = decoder(generator, style_space_B, latent_B, noise_B)\n",
    "visual(image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447dd61c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
